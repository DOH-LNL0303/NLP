---
title: "TidyText Learning"
author: "Lareina La Flair"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    number_sections: true
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

packages <- c("dplyr", "janeaustenr", "tidytext", "textdata", "wesanderson", "forcats", "plotly", "gutenbergr", "stringr", "tidyr", "SnowballC", "wordcloud", "topicmodels", "data.table", "purrr")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
```

# TidyText Mining

This document follows exercises in the book [Text Mining with R](https://www.tidytextmining.com/tidytext.html)


The tidy text format as being a table with one-token-per-row.

```{r }
```

## Sentiment Analysis

Three general purpose lexicons:

*AFINN from Finn Årup Nielsen
*bing from Bing Liu and collaborators
*nrc from Saif Mohammad and Peter Turney

"All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment."

```{r }
```

## Analyzing word and document frequency: tf-idf

Another approach is to look at a term’s inverse document frequency (idf), 
which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. This can be combined with term frequency to calculate a term’s tf-idf (the two quantities multiplied together), 

*The frequency of a term adjusted for how rarely it is used.
* idf decreases in accordance to how common a word is among a corpus of documents

```{r tidy1, eval=TRUE}

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE)

total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words


ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")

freq_by_rank <- book_words %>% 
  group_by(book) %>% #already sorted in order of desc freq
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

freq_by_rank

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_abline(intercept = -0.62, slope = -1.1, 
              color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

book_tf_idf <- book_words %>%
  bind_tf_idf(word, book, n)

book_tf_idf %>%
  select(-total) %>%
  arrange(desc(tf_idf))


p <- book_tf_idf %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL) +
  #scale_fill_manual(values=wes_palette(n=6, name="GrandBudapest1")) 
  scale_fill_manual(values = wes_palette(6, name = "Zissou1", type = "continuous"))

ggplotly(p)

#################

darwin <- gutenberg_works(author == "Darwin, Charles")

darwin_books <- gutenberg_download(c(944, 1227, 2087, 2010), 
                              meta_fields = "title")

colnames(darwin_books) <- stringr::str_replace_all(colnames(darwin_books),"[:punct:]"," ")

darwin_words <- darwin_books %>%
  unnest_tokens(word, text) %>%
  count(title, word, sort = TRUE)

plot_darwin <- darwin_words %>%
  bind_tf_idf(word, title, n) %>%
  mutate(title = factor(
    title,
    levels =
      c(
        "The Voyage of the Beagle",
        "The Expression of the Emotions in Man and Animals",
        "Life and Letters of Charles Darwin — Volume 1",
        "The Autobiography of Charles Darwin"
      )
  ))

plot_darwin %>% 
  group_by(title) %>% 
  slice_max(tf_idf, n = 15) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = title)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~title, ncol = 2, scales = "free") +
  scale_fill_manual(values=wes_palette(n=4, name="GrandBudapest1")) 

library(stringr)

darwin_books %>% 
  filter(str_detect(text, "eyebrows")) %>% 
  select(text)

bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf

# plot

a <- bigram_tf_idf %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL) +
  #scale_fill_manual(values=wes_palette(n=6, name="GrandBudapest1")) 
  scale_fill_manual(values = wes_palette(6, name = "Zissou1", type = "continuous"))

ggplotly(a)


```

## Relationships between words: n-grams and correlations

"Tokenize into consecutive sequences of words, called n-grams. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them."

Add the token = "ngrams" option to unnest_tokens(), and setting n to the number of words we wish to capture in each n-gram. When we set n to 2, we are examining pairs of two consecutive words, often called “bigrams”

“separate/filter/count/unite” let us find the most common bigrams not containing stop-words.

```{r}

```

### N grams and sentiment analysis with negations

Bigram, trigrams, and visualizing with `igraph`

```{r ngrams, eval=TRUE, echo=FALSE}
# N-grams!

austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams

austen_bigrams %>%
  count(bigram, sort = TRUE)

# Separate into 2 words

bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# Filter out  stop words

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) # stop words here is 1000+ words

# Count: new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

# “separate/filter/count/unite” let us find the most common bigrams not containing stop-words.

# Unite: bring them back together!
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united

# Tri-grams!
# Note that when using count you lose the grouping (group_by) variable, here book

austen_books() %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)

# Analyze

# most common mentions of street in books?

bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)

# Sentiment analysis with negations

bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)

# how often sentiment-associated words are preceded by “not” or other negating words.

# gives a numeric sentiment value for each word, 
#with positive or negative numbers indicating the direction of the sentiment.

library(tidytext)

AFINN <- get_sentiments("afinn")

AFINN

not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE) #summarizes by word2 to give word, sentiment value, and freq

# multiply their value by the number of times they appear 
#(so that a word with a value of +3 occurring 10 times has as much impact as a 
# word with a sentiment value of +1 occurring 30 times). We visualize the result with a bar plot

library(ggplot2)

not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"") +
  scale_fill_manual(values = rev(wes_palette(6, name = "Zissou1", type = "continuous")))


# why not replace n+ value with "contribution" ?
# same result

not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>% #sort by absolute value
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(contribution, word2, fill = contribution > 0)) + #splits contribution into + and -
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"") +
  scale_fill_manual(values = rev(wes_palette(6, name = "Zissou1", type = "continuous")))

# Visualizing a network of bigrams with ggraph
library(igraph)

# original counts
bigram_counts

# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

# graph_from_data_frame() function takes a data frame of edges with columns 
#for “from”, “to”, and edge attributes (in this case n):

bigram_graph

# for a basic graph we need to add three layers: nodes, edges, and text.

library(ggraph)
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

# Figure 4.4: Common bigrams in Jane Austen’s novels, 
#showing those that occurred more than 20 times and where neither word was a stop word

# Prettier and more informative
# add directionality with an arrow,

set.seed(2020)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = T,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "plum3", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

#### Formula for visualizing bigrams in any text!

```{r ngramsform, eval=TRUE, echo=FALSE}
# libraries

library(dplyr)
library(tidyr)
library(tidytext)
library(ggplot2)
library(igraph)
library(ggraph)

# create bigrams function

count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

# visualizing bigrams function

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "plum3", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}
```


```{r kjv}
#######################

# the King James version is book 10 on Project Gutenberg:
library(gutenbergr)
kjv <- gutenberg_download(10)

library(stringr)

kjv_bigrams <- kjv %>%
  count_bigrams() # function created above

# filter out rare combinations, as well as digits
kjv_bigrams %>%
  filter(n > 40,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
  visualize_bigrams() # function created above


```

### Correlations 

Counting and correlating pairs of words with the `widyr` package

Most operations for finding pairwise counts or correlations

need to turn the data into a wide matrix first.

```{r correlations, eval = TRUE, echo=FALSE}
# What words tend to appear within the same section?

austen_section_words <- austen_books() %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word)

austen_section_words

library(widyr)

# count words co-occuring within sections
word_pairs <- austen_section_words %>%
  pairwise_count(word, section, sort = TRUE)

word_pairs

# The pairwise_cor() function in widyr lets 
# us find the phi coefficient between words based on how often they appear in the same section.

word_cors <- austen_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

word_cors

# Words from Pride and Prejudice that 
# were most correlated with ‘elizabeth’, ‘pounds’, ‘married’, and ‘pride’

word_cors %>%
  filter(item1 %in% c("elizabeth", "pounds", "married", "pride")) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()

# bigrams

set.seed(2016)

word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "plum3", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

#Document Term Matrices

Tidying DocumentTermMatrix objects with the tm package

The broom package introduced the `tidy()` verb, which takes a non-tidy object and turns it into a tidy data frame. **Tidy** the corpus into document-term-count df!

Used [Kaggle source data](https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format) for [movie review analysis](https://rstudio-pubs-static.s3.amazonaws.com/132792_864e3813b0ec47cb95c7e1e2e2ad83e7.html)

A note on sparsity: sparsity is smaller as it approaches 1.0. `removeSparseTerms()`, sparsity refers to the threshold of relative document frequency for a term, above which the term will be removed. A threshold of .99 means that probably all terms will be retained.

```{r dtm, eval = TRUE, echo=FALSE}
#### Tidy text and sentiment

library(tm)
library(SnowballC)
library(wordcloud)

data("AssociatedPress", package = "topicmodels")
AssociatedPress

# Notice that this DTM is 99% sparse (99% of document-word pairs are zero)

# access terms

terms <- Terms(AssociatedPress)
head(terms)

# turn into a data frame with one-token-per-document-per-row

library(dplyr)
library(tidytext)

ap_td <- tidy(AssociatedPress) # gives document, term, and count. Similar to melt()
ap_td

# in this form, you can do sentiment analysis and use tidytext and ggplot

# add sentiments to words in corpus

ap_sentiments <- ap_td %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))

ap_sentiments

# visualize words most frequently contributing to positive or negative sentiment

library(ggplot2)

ap_sentiments %>%
  count(sentiment, term, wt = count) %>% # sentiment, term, n
  ungroup() %>%
  filter(n >= 200) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>% #assigns neg. value to neg. sentiments
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(n, term, fill = sentiment)) +
  geom_col() +
  labs(x = "Contribution to sentiment", y = NULL) +
  scale_fill_manual(values = rev(wes_palette(2, name = "Zissou1", type = "continuous")))

########################################

# Tidying document feature matrices (dfm) with quanteda package

# tokenizing with quanteda, an unnest tokens alternative

data("data_corpus_inaugural", package = "quanteda")

inaug_dfm <- data_corpus_inaugural %>%
  quanteda::tokens() %>%
  quanteda::dfm(verbose = FALSE)

inaug_dfm

#> Document-feature matrix of: 59 documents, 9,439 features (91.84% sparse) and 4 docvars.

# tidy the corpus into document-term-count

inaug_td <- tidy(inaug_dfm)
inaug_td

# now calculate tf-idf

inaug_tf_idf <- inaug_td %>%
  bind_tf_idf(term, document, count) %>%
  arrange(desc(tf_idf))

inaug_tf_idf

# visualize how terms change in frequency of use over time
# extract() desired values and turns into new column. Here, year_total
# complete() Turns implicit missing values into explicit missing values

library(tidyr)

year_term_counts <- inaug_td %>%
  extract(document, "year", "(\\d+)", convert = TRUE) %>% # extract year with regex digit
  complete(year, term, fill = list(count = 0)) %>% # excplicit 0s
  group_by(year) %>%
  mutate(year_total = sum(count))

# the rise of "America" "freedom" and "God"

year_term_counts %>%
  filter(term %in% c("god", "america", "foreign", "union", "constitution", "freedom")) %>%
  ggplot(aes(year, count / year_total)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~ term, scales = "free_y") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(y = "% frequency of word in inaugural address") 



########################################

## Movie review exercise and word clouds

reviews = fread("data/train.csv", stringsAsFactors = F)

#reviews = fread("review_polarity.tar.gz")
# untar("review_polarity.tar.gz", list=TRUE)  # this extracts the file from archive
# reviews <- read.table("review_polarity.tar.gz") 

# Transform to corpus
review_corpus = Corpus(VectorSource(reviews$text))
#
# Normalize the texts in the reviews using a series of pre-processing steps: 1. Switch to lower case 2. Remove numbers 3. Remove punctuation marks and stopwords 4. Remove extra whitespaces

review_corpus = tm_map(review_corpus, content_transformer(tolower))
review_corpus = tm_map(review_corpus, removeNumbers)
review_corpus = tm_map(review_corpus, removePunctuation)
review_corpus = tm_map(review_corpus, removeWords, c("the", "and", stopwords("english")))
review_corpus =  tm_map(review_corpus, stripWhitespace)

# See what it looks like

inspect(review_corpus[1])

# To analyze the textual data, we use a Document-Term Matrix (DTM) representation: 
# documents as the rows, terms/words as the columns, frequency of the term in the document as the entries.

review_dtm <- DocumentTermMatrix(review_corpus)
review_dtm

inspect(review_dtm[500:505, 500:505])

# To reduce the dimension of the DTM, we can remove the less frequent terms such that the sparsity is less than 0.95

review_dtm = removeSparseTerms(review_dtm, 0.99)
review_dtm

# check first document, words 1-20

inspect(review_dtm[1,1:20]) 

# word cloud prep

findFreqTerms(review_dtm, 1000)

freq = data.frame(sort(colSums(as.matrix(review_dtm)), decreasing=TRUE))
png("movierevs_wordcloud.png", width=1280,height=800) #save as png
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(8, "Dark2"))
dev.off()

# Not terribly informative (one, film, movie, etc.) Solution: Use tf-idf to improve relative importance of word

review_dtm_tfidf <- DocumentTermMatrix(review_corpus, control = list(weighting = weightTfIdf))
review_dtm_tfidf = removeSparseTerms(review_dtm_tfidf, 0.99)
review_dtm_tfidf

# Inspect the first document
inspect(review_dtm_tfidf[1,1:20])

# Is new wordcloud more informative? (A: somewhat!)

freq = data.frame(sort(colSums(as.matrix(review_dtm_tfidf)), decreasing=TRUE))
png("movierevs_wordcloud_tfidf.png", width=1280,height=800)
wordcloud(rownames(freq), freq[,1], max.words=100, colors=brewer.pal(8, "Dark2"))
```
 
## Examples of Mining - Twitter

Tweet analysis! I'm a dubious Twitter user, but I was curious to practice text mining with a corpus of tweets from 2019-2020. First step is to download the data and distribution of tweets.
Directions to download your Twitter archive [here](https://help.twitter.com/en/managing-your-account/how-to-download-your-twitter-archive).

```{r examples, eval = TRUE, echo=FALSE}

library(lubridate)
library(ggplot2)
library(dplyr)
library(readr)

tweets_ <- read_csv("data/tweets_lareina.csv")

tweets <- tweets %>%
  mutate(timestamp = ymd_hms(timestamp))

ggplot(tweets, aes(x = timestamp, fill = person)) +
  geom_histogram(position = "identity", bins = 20, show.legend = FALSE) 

# Word frequencies

library(tidytext)
library(stringr)

remove_reg <- "&amp;|&lt;|&gt;"
tidy_tweets <- tweets %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_remove_all(text, remove_reg)) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))

frequency <- tidy_tweets %>% 
  count(person, word, sort = TRUE) %>% 
  left_join(tidy_tweets %>% 
              count(person, name = "total")) %>%
  mutate(freq = n/total)

frequency

# reshape

library(tidyr)

frequency <- frequency %>% 
  select(person, word, freq) %>% 
  pivot_wider(names_from = person, values_from = freq) %>%
  arrange(Julia, David)

frequency

# plot, if comparing

library(scales)

ggplot(frequency, aes(Julia, David)) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  geom_abline(color = "red")

# filter timestamp

tidy_tweets <- tidy_tweets %>%
  filter(timestamp >= as.Date("2016-01-01"),
         timestamp < as.Date("2017-01-01"))

# words by time

words_by_time <- tidy_tweets %>%
  filter(!str_detect(word, "^@")) %>%
  mutate(time_floor = floor_date(timestamp, unit = "1 month")) %>%
  count(time_floor, person, word) %>%
  group_by(person, time_floor) %>%
  mutate(time_total = sum(n)) %>%
  group_by(person, word) %>%
  mutate(word_total = sum(n)) %>%
  ungroup() %>%
  rename(count = n) %>%
  filter(word_total > 30)

words_by_time

nested_data <- words_by_time %>%
  nest(data = c(-word, -person)) 

nested_data

library(purrr)

nested_models <- nested_data %>%
  mutate(models = map(data, ~ glm(cbind(count, time_total) ~ time_floor, ., 
                                  family = "binomial")))

nested_models

library(broom)

slopes <- nested_models %>%
  mutate(models = map(models, tidy)) %>%
  unnest(cols = c(models)) %>%
  filter(term == "time_floor") %>%
  mutate(adjusted.p.value = p.adjust(p.value))

# which words have changed significantly

top_slopes <- slopes %>% 
  filter(adjusted.p.value < 0.05)

top_slopes

words_by_time %>%
  inner_join(top_slopes, by = c("word", "person")) %>%
  filter(person == "David") %>%
  ggplot(aes(time_floor, count/time_total, color = word)) +
  geom_line(size = 1.3) +
  labs(x = NULL, y = "Word frequency")
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
